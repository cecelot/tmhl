@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

% not published, not on openreview either
@misc{mckenna2023sources,
      title={Sources of Hallucination by Large Language Models on Inference Tasks}, 
      author={Nick McKenna and Tianyi Li and Liang Cheng and Mohammad Javad Hosseini and Mark Johnson and Mark Steedman},
      year={2023},
      eprint={2305.14552},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% **THE PAPER WAS ACCEPTED** can't find a non-openreivew source
@article{duan2023shifting,
  title={Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models},
  author={Duan, Jinhao and Cheng, Hao and Wang, Shiqi and Wang, Chenan and Zavalny, Alex and Xu, Renjing and Kailkhura, Bhavya and Xu, Kaidi},
  journal={arXiv preprint arXiv:2307.01379},
  year={2023}
}

% **THE PAPER WAS ACCEPTED** presented at ICLR 2023, doesn't seem to have a non-arxiv link.
@misc{kuhn2023semantic,
      title={Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation}, 
      author={Lorenz Kuhn and Yarin Gal and Sebastian Farquhar},
      year={2023},
      eprint={2302.09664},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% search again for paper later and see if published, if can't find publication, ok
@misc{scidebug,
  title         = {Explainable Automated Debugging via Large Language Model-driven Scientific Debugging},
  author        = {Sungmin Kang and Bei Chen and Shin Yoo and Jian-Guang Lou},
  year          = {2023},
  eprint        = {2304.02195},
  archiveprefix = {arXiv},
  primaryclass  = {cs.SE}
}

@article{mixcl,
    title={Contrastive Learning Reduces Hallucination in Conversations}, 
    volume={37}, 
    url={https://ojs.aaai.org/index.php/AAAI/article/view/26596}, 
    DOI={10.1609/aaai.v37i11.26596}, 
    number={11}, 
    journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
    author={Sun, Weiwei and Shi, Zhengliang and Gao, Shen and Ren, Pengjie and de Rijke, Maarten and Ren, Zhaochun}, 
    year={2023},
    pages={13618-13626}
}

@misc{shazeer2017outrageously,
      title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}, 
      author={Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
      year={2017},
      eprint={1701.06538},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{gpt4moe, 
    title={Is GPT-4 Just Eight Smaller Models},
    author={Matt Rickard},
    year={2023},
    url={https://matt-rickard.com/mixture-of-experts-is-gpt-4-just-eight-smaller-models}
}

@misc{statistachatgpt,
    title={Time to One Million Users},
    author={Katharina Buchholz},
    year={2023},
    url={https://www.statista.com/chart/29174/time-to-one-million-users/}
}