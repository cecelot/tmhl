### "Improving Transparency and Mitigating Hallucinations in LLMs."

A proposal for modifying LLM training processes using emerging techniques to improve transparency and mitigate hallucinations.

**Abstract:** Large language models (LLMs) have impressive performance in parsing language and generating natural-sounding responses. However, LLMs have several problems related to the accuracy of those responses. They may "hallucinate" (provide incorrect or irrelevant information), provide confidently incorrect responses and explanations, and may not sufficiently explain their reasoning. This paper explores existing solutions to these problems, such as Chain-of-Thought prompting to improve explainability, calculations to measure uncertainty in responses (called "semantic uncertainty"), and Mixed-contrastive Learning to reduce the generation of hallucinations. We then combine several of these solutions in a proposal which modifies the structure of the GPT-series of models in an attempt to simultaneously reduce hallucinations, use uncertainty in choosing LLM responses, and increase explainability.

### Credits

This paper was written by me, with advising by [Andrew F.](https://www.polygence.org/mentors/22382/andrew) from [Polygence](https://polygence.org).
